{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8m7bzAJCmYd"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CENG442 Assignment 1 - Azerbaijani Text Preprocessing + Word Embeddings\n",
        "Made by:\n",
        "Ege √úndeni≈ü\n",
        "Emine Sena Top\n",
        "R√ºmeysa Yavuzkanat\n",
        "\"\"\"\n",
        "\n",
        "# --- Core imports ---\n",
        "import re, html, unicodedata\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Data handling ---\n",
        "import pandas as pd\n",
        "\n",
        "# --- Text cleaning ---\n",
        "try:\n",
        "    from ftfy import fix_text\n",
        "except Exception:\n",
        "    def fix_text(s): return s\n",
        "\n",
        "# --- Embeddings ---\n",
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "# --- Utilities ---\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRXR6gXaSYKh",
        "outputId": "c46dab91-7b0c-41ef-a2f0-4ddb812b2a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: labeled-sentiment_2col.xlsx (rows=2955)\n",
            "Saved: test__1__2col.xlsx (rows=4198)\n",
            "Saved: train__3__2col.xlsx (rows=19557)\n",
            "Saved: train-00000-of-00001_2col.xlsx (rows=41756)\n",
            "Saved: merged_dataset_CSV__1__2col.xlsx (rows=55662)\n",
            "Wrote corpus_all.txt with 124353 lines\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import re, html, unicodedata\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from ftfy import fix_text\n",
        "except Exception:\n",
        "    def fix_text(s): return s\n",
        "\n",
        "# Azerbaijani-aware lowercase\n",
        "def lower_az(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    s = s.replace(\"I\", \"ƒ±\").replace(\"ƒ∞\", \"i\")\n",
        "    s = s.lower().replace(\"i Ãá\", \"i\")\n",
        "    return s\n",
        "\n",
        "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "URL_RE      = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
        "EMAIL_RE    = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", re.IGNORECASE)\n",
        "PHONE_RE    = re.compile(r\"\\+?\\d[\\d\\-\\s\\(\\)]{6,}\\d\")\n",
        "USER_RE     = re.compile(r\"@\\w+\")\n",
        "MULTI_PUNCT = re.compile(r\"([!?.,;:])\\1{1,}\")\n",
        "MULTI_SPACE = re.compile(r\"\\s+\")\n",
        "REPEAT_CHARS= re.compile(r\"(.)\\1{2,}\", flags=re.UNICODE)\n",
        "\n",
        "TOKEN_RE = re.compile(\n",
        "    r\"[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+(?:'[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+)?\"\n",
        "    r\"|<NUM>|URL|EMAIL|PHONE|USER|EMO_(?:POS|NEG)\"\n",
        ")\n",
        "\n",
        "EMO_MAP = {\"üôÇ\":\"EMO_POS\",\"üòÄ\":\"EMO_POS\",\"üòç\":\"EMO_POS\",\"üòä\":\"EMO_POS\",\"üëç\":\"EMO_POS\",\n",
        "           \"‚òπ\":\"EMO_NEG\",\"üôÅ\":\"EMO_NEG\",\"üò†\":\"EMO_NEG\",\"üò°\":\"EMO_NEG\",\"üëé\":\"EMO_NEG\",\n",
        "           \"üî•\":\"EMO_POS\", \"üò≠\":\"EMO_NEG\"}\n",
        "\n",
        "SLANG_MAP = {\"slm\":\"salam\",\"tmm\":\"tamam\",\"sagol\":\"saƒüol\",\"cox\":\"√ßox\",\"yaxsi\":\"yax≈üƒ±\"}\n",
        "NEGATORS  = {\"yox\",\"deyil\",\"he√ß\",\"q…ôtiyy…ôn\",\"yoxdur\"}\n",
        "\n",
        "# --- Domain helpers ---\n",
        "NEWS_HINTS   = re.compile(r\"\\b(apa|trend|azertac|reuters|bloomberg|dha|aa|bbc|cnn)\\b\", re.I)\n",
        "SOCIAL_HINTS = re.compile(r\"\\b(rt)\\b|@|#|(?:üòÇ|üòç|üòä|üëç|üëé|üò°|üôÇ)\")\n",
        "REV_HINTS    = re.compile(r\"\\b(azn|manat|qiym…ôt|aldƒ±m|ulduz|√ßox yax≈üƒ±|√ßox pis)\\b\", re.I)\n",
        "PRICE_RE     = re.compile(r\"\\b\\d+\\s*(azn|manat)\\b\", re.I)\n",
        "STARS_RE     = re.compile(r\"\\b([1-5])\\s*ulduz\\b\", re.I)\n",
        "POS_RATE     = re.compile(r\"\\b√ßox yax≈üƒ±\\b\")\n",
        "NEG_RATE     = re.compile(r\"\\b√ßox pis\\b\")\n",
        "\n",
        "def detect_domain(text: str) -> str:\n",
        "    s = text.lower()\n",
        "    if NEWS_HINTS.search(s): return \"news\"\n",
        "    if SOCIAL_HINTS.search(s): return \"social\"\n",
        "    if REV_HINTS.search(s):   return \"reviews\"\n",
        "    return \"general\"\n",
        "\n",
        "def domain_specific_normalize(cleaned: str, domain: str) -> str:\n",
        "    if domain == \"reviews\":\n",
        "        s = PRICE_RE.sub(\" <PRICE> \", cleaned)\n",
        "        s = STARS_RE.sub(lambda m: f\" <STARS_{m.group(1)}> \", cleaned)\n",
        "        s = POS_RATE.sub(\" <RATING_POS> \", s)\n",
        "        s = NEG_RATE.sub(\" <RATING_NEG> \", s)\n",
        "        return \" \".join(s.split())\n",
        "    return cleaned\n",
        "\n",
        "def add_domain_tag(line: str, domain: str) -> str:\n",
        "    return f\"dom{domain} \" + line  # no punctuation\n",
        "\n",
        "def normalize_text_az(s: str, numbers_to_token=True, keep_sentence_punct=False) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    for emo, tag in EMO_MAP.items():\n",
        "        s = s.replace(emo, f\" {tag} \")\n",
        "    s = fix_text(s)\n",
        "    s = html.unescape(s)\n",
        "    s = HTML_TAG_RE.sub(\" \", s)\n",
        "    s = URL_RE.sub(\" URL \", s)\n",
        "    s = EMAIL_RE.sub(\" EMAIL \", s)\n",
        "    s = PHONE_RE.sub(\" PHONE \", s)\n",
        "    s = re.sub(r\"#([A-Za-z0-9_]+)\", lambda m: \" \" + re.sub('([a-z])([A-Z])', r'\\1 \\2', m.group(1)) + \" \", s)\n",
        "    s = USER_RE.sub(\" USER \", s)\n",
        "    s = lower_az(s)\n",
        "    s = MULTI_PUNCT.sub(r\"\\1\", s)\n",
        "    if numbers_to_token:\n",
        "        s = re.sub(r\"\\d+\", \" <NUM> \", s)\n",
        "    if keep_sentence_punct:\n",
        "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ.!?]\", \" \", s)\n",
        "    else:\n",
        "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", s)\n",
        "    s = MULTI_SPACE.sub(\" \", s).strip()\n",
        "    toks = TOKEN_RE.findall(s)\n",
        "    norm = []\n",
        "    mark_neg = 0\n",
        "    for t in toks:\n",
        "        t = REPEAT_CHARS.sub(r\"\\1\\1\", t)\n",
        "        t = SLANG_MAP.get(t, t)\n",
        "        if t in NEGATORS:\n",
        "            norm.append(t); mark_neg = 3; continue\n",
        "        if mark_neg > 0 and t not in {\"URL\",\"EMAIL\",\"PHONE\",\"USER\"}:\n",
        "            norm.append(t + \"_NEG\"); mark_neg -= 1\n",
        "        else:\n",
        "            norm.append(t)\n",
        "    norm = [t for t in norm if not (len(t) == 1 and t not in {\"o\",\"e\"})]\n",
        "    return \" \".join(norm).strip()\n",
        "\n",
        "def map_sentiment_value(v, scheme: str):\n",
        "    if scheme == \"binary\":\n",
        "        try: return 1.0 if int(v) == 1 else 0.0\n",
        "        except Exception: return None\n",
        "    s = str(v).strip().lower()\n",
        "    if s in {\"pos\",\"positive\",\"1\",\"m√ºsb…ôt\",\"good\",\"pozitiv\"}: return 1.0\n",
        "    if s in {\"neu\",\"neutral\",\"2\",\"neytral\"}: return 0.5\n",
        "    if s in {\"neg\",\"negative\",\"0\",\"m…ônfi\",\"bad\",\"neqativ\"}: return 0.0\n",
        "    return None\n",
        "\n",
        "def process_file(in_path, text_col, label_col, scheme, out_two_col_path, remove_stopwords=False):\n",
        "    df = pd.read_excel(in_path)\n",
        "    for c in [\"Unnamed: 0\",\"index\"]:\n",
        "        if c in df.columns: df = df.drop(columns=[c])\n",
        "    assert text_col in df.columns and label_col in df.columns\n",
        "    df = df.dropna(subset=[text_col])\n",
        "    df = df[df[text_col].astype(str).str.strip().str.len() > 0]\n",
        "    df = df.drop_duplicates(subset=[text_col])\n",
        "\n",
        "    df[\"cleaned_text\"] = df[text_col].astype(str).apply(lambda s: normalize_text_az(s))\n",
        "    df[\"__domain__\"] = df[text_col].astype(str).apply(detect_domain)\n",
        "    df[\"cleaned_text\"] = df.apply(lambda r: domain_specific_normalize(r[\"cleaned_text\"], r[\"__domain__\"]), axis=1)\n",
        "\n",
        "    if remove_stopwords:\n",
        "        sw = set([\"v…ô\",\"il…ô\",\"amma\",\"ancaq\",\"lakin\",\"ya\",\"h…ôm\",\"ki\",\n",
        "                  \"bu\",\"bir\",\"o\",\"biz\",\"siz\",\"m…ôn\",\"s…ôn\",\"orada\",\"burada\",\n",
        "                  \"b√ºt√ºn\",\"h…ôr\",\"artƒ±q\",\"√ßox\",\"az\",\"…ôn\",\"d…ô\",\"da\",\"√º√ß√ºn\",\n",
        "                  \"deyildir\", \"el…ô\", \"√∂z\", \"onun\", \"…ôslind…ô\", \"buna\", \"g√∂r…ô\"])\n",
        "        for keep in [\"deyil\",\"yox\",\"he√ß\",\"q…ôtiyy…ôn\",\"yoxdur\"]:\n",
        "            sw.discard(keep)\n",
        "        df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda s: \" \".join([t for t in s.split() if t not in sw]))\n",
        "\n",
        "    df[\"sentiment_value\"] = df[label_col].apply(lambda v: map_sentiment_value(v, scheme))\n",
        "    df = df.dropna(subset=[\"sentiment_value\"])\n",
        "    df[\"sentiment_value\"] = df[\"sentiment_value\"].astype(float)\n",
        "\n",
        "    out_df = df[[\"cleaned_text\",\"sentiment_value\"]].reset_index(drop=True)\n",
        "    Path(out_two_col_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_df.to_excel(out_two_col_path, index=False)\n",
        "    print(f\"Saved: {out_two_col_path} (rows={len(out_df)})\")\n",
        "\n",
        "def build_corpus_txt(input_files, text_cols, out_txt=\"corpus_all.txt\"):\n",
        "    lines = []\n",
        "    for (f, text_col) in zip(input_files, text_cols):\n",
        "        df = pd.read_excel(f)\n",
        "        for raw in df[text_col].dropna().astype(str):\n",
        "            dom = detect_domain(raw)\n",
        "            s = normalize_text_az(raw, keep_sentence_punct=True)\n",
        "            parts = re.split(r\"[.!?]+\", s)\n",
        "            for p in parts:\n",
        "                p = p.strip()\n",
        "                if not p: continue\n",
        "                p = re.sub(r\"[^\\w\\s…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", p)\n",
        "                p = \" \".join(p.split()).lower()\n",
        "                if p:\n",
        "                    lines.append(f\"dom{dom} \" + p)\n",
        "    with open(out_txt, \"w\", encoding=\"utf-8\") as w:\n",
        "        for ln in lines:\n",
        "            w.write(ln + \"\\n\")\n",
        "    print(f\"Wrote {out_txt} with {len(lines)} lines\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    CFG = [\n",
        "        (\"sample_data/labeled-sentiment.xlsx\",        \"text\", \"sentiment\", \"tri\"),\n",
        "        (\"sample_data/test__1_.xlsx\",                 \"text\", \"label\",     \"binary\"),\n",
        "        (\"sample_data/train__3_.xlsx\",                \"text\", \"label\",     \"binary\"),\n",
        "        (\"sample_data/train-00000-of-00001.xlsx\",     \"text\", \"labels\",    \"tri\"),\n",
        "        (\"sample_data/merged_dataset_CSV__1_.xlsx\",   \"text\", \"labels\",    \"binary\"),\n",
        "    ]\n",
        "    for fname, tcol, lcol, scheme in CFG:\n",
        "        out = f\"{Path(fname).stem}_2col.xlsx\"\n",
        "        process_file(fname, tcol, lcol, scheme, out, remove_stopwords=False)\n",
        "        #process_file(fname, tcol, lcol, scheme, out, remove_stopwords=True)\n",
        "    build_corpus_txt([c[0] for c in CFG], [c[1] for c in CFG], out_txt=\"corpus_all.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2tecPtLSbLc",
        "outputId": "27e1ea83-3aa9-4749-d09a-9bc182a0b107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved embeddings.\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "files = [\n",
        "    \"labeled-sentiment_2col.xlsx\",\n",
        "    \"test__1__2col.xlsx\",\n",
        "    \"train__3__2col.xlsx\",\n",
        "    \"train-00000-of-00001_2col.xlsx\",\n",
        "    \"merged_dataset_CSV__1__2col.xlsx\",\n",
        "]\n",
        "\n",
        "sentences = []\n",
        "for f in files:\n",
        "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
        "    sentences.extend(df[\"cleaned_text\"].astype(str).str.split().tolist())\n",
        "\n",
        "Path(\"embeddings\").mkdir(exist_ok=True)\n",
        "w2v = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=3, sg=1, negative=10, epochs=10)\n",
        "w2v.save(\"embeddings/word2vec.model\")\n",
        "\n",
        "ft  = FastText(sentences=sentences, vector_size=300, window=5, min_count=3, sg=1, min_n=3, max_n=6, epochs=10)\n",
        "ft.save(\"embeddings/fasttext.model\")\n",
        "\n",
        "print(\"Saved embeddings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjmVnz0PSf3t",
        "outputId": "e8f88418-3e08-442d-9c58-a758f937b799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Lexical coverage (per dataset) ==\n",
            "labeled-sentiment_2col.xlsx: W2V=0.932, FT(vocab)=0.932\n",
            "test__1__2col.xlsx: W2V=0.987, FT(vocab)=0.987\n",
            "train__3__2col.xlsx: W2V=0.990, FT(vocab)=0.990\n",
            "train-00000-of-00001_2col.xlsx: W2V=0.943, FT(vocab)=0.943\n",
            "merged_dataset_CSV__1__2col.xlsx: W2V=0.949, FT(vocab)=0.949\n",
            "\n",
            "== Similarity (higher better for synonyms; lower better for antonyms) ==\n",
            "Synonyms: W2V=0.341, FT=0.433\n",
            "Antonyms: W2V=0.358, FT=0.419\n",
            "Separation (Syn - Ant): W2V=-0.017, FT=0.014\n",
            "\n",
            "== Nearest neighbors (qualitative) ==\n",
            "  W2V NN for 'yax≈üƒ±': ['iyi', '<RATING_POS>', 'yaxshi', 'yaxwi', 'yaxsƒ±']\n",
            "  FT  NN for 'yax≈üƒ±': ['yax≈üƒ±ƒ±', 'yax≈üƒ±kƒ±', 'yax≈üƒ±ca', 'yax≈ü', 'yax≈üƒ±ya']\n",
            "  W2V NN for 'pis': ['g√ºnd', '√∂rn…ôk', 'bakƒ±kartƒ±', 'yax≈üƒ±dƒ±r_NEG', 'karde≈üi']\n",
            "  FT  NN for 'pis': ['piis', 'pisdii', 'pi', 'pisi', 'pisdi']\n",
            "  W2V NN for '√ßox': ['√ßoox', 'b…ôy…ônilsin', '√ß√∂x', 'g√∂z…ôldir', '√ßoxx']\n",
            "  FT  NN for '√ßox': ['√ßox√ßox', '√ßoxx', '√ßoxh', '√ßoh', '√ßo']\n",
            "  W2V NN for 'bahalƒ±': ['portretlerin…ô', 'yaxtalarƒ±', 'villalarƒ±', 'metallarla', 'unisin…ô']\n",
            "  FT  NN for 'bahalƒ±': ['bahalƒ±ƒ±', 'bahalƒ±sƒ±', 'bahalƒ±q', 'baharlƒ±', 'pahalƒ±']\n",
            "  W2V NN for 'ucuz': ['d√ºz…ôltdirilib', '≈üeytanbazardan', 'qiymete', 'elektik', 'sududu']\n",
            "  FT  NN for 'ucuz': ['ucuzu', 'ucuza', 'ucuzdu', 'ucuzluƒüa', 'ucuzdur']\n",
            "  W2V NN for 'm√ºk…ômm…ôl': ['m√∂ht…ô≈ü…ômm', 'bayƒ±ldƒ±m', 'muk…ômm…ôl', '√∂yr…ôdici', '≈üirinsu']\n",
            "  FT  NN for 'm√ºk…ômm…ôl': ['m√ºk…ômm…ôll', 'm√ºk…ôm…ôl', 'muk…ômm…ôl', 'm√ºk…ômm…ôldi', 'm√ºk…ômm…ôlsiz']\n",
            "  W2V NN for 'd…ôh≈ü…ôt': ['xal√ßalardan', 'birda', 'ayranlarƒ±', 't…ôsirlidi', 'kreditl…ôrini']\n",
            "  FT  NN for 'd…ôh≈ü…ôt': ['d…ôh≈ü…ôtd√º', 'd…ôh≈ü…ôt…ô', 'd…ôh≈ü…ôti', 'd…ôh≈ü…ôtizm', 'd…ôh≈ü…ôtdi']\n",
            "  W2V NN for '<PRICE>': []\n",
            "  FT  NN for '<PRICE>': ['engiltdere', 'reeceep', 'recebzade', 'flight', 'light']\n",
            "  W2V NN for '<RATING_POS>': ['deneyin', 's√ºper', 'uygulama', 'internetli', 'superdir']\n",
            "  FT  NN for '<RATING_POS>': ['<RATING_NEG>', 's√ºperr', '√ßookk', 'c√∂x', '√ßokk']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "w2v = Word2Vec.load(\"embeddings/word2vec.model\")\n",
        "ft  = FastText.load(\"embeddings/fasttext.model\")\n",
        "\n",
        "seed_words = [\"yax≈üƒ±\",\"pis\",\"√ßox\",\"bahalƒ±\",\"ucuz\",\"m√ºk…ômm…ôl\",\"d…ôh≈ü…ôt\",\"<PRICE>\",\"<RATING_POS>\"]\n",
        "syn_pairs  = [(\"yax≈üƒ±\",\"…ôla\"), (\"bahalƒ±\",\"qiym…ôtli\"), (\"ucuz\",\"s…ôrf…ôli\")]\n",
        "ant_pairs  = [(\"yax≈üƒ±\",\"pis\"), (\"bahalƒ±\",\"ucuz\")]\n",
        "\n",
        "def lexical_coverage(model, tokens):\n",
        "    vocab = model.wv.key_to_index\n",
        "    return sum(1 for t in tokens if t in vocab) / max(1,len(tokens))\n",
        "\n",
        "files = [\n",
        "    \"labeled-sentiment_2col.xlsx\",\n",
        "    \"test__1__2col.xlsx\",\n",
        "    \"train__3__2col.xlsx\",\n",
        "    \"train-00000-of-00001_2col.xlsx\",\n",
        "    \"merged_dataset_CSV__1__2col.xlsx\",\n",
        "]\n",
        "\n",
        "def read_tokens(f):\n",
        "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
        "    return [t for row in df[\"cleaned_text\"].astype(str) for t in row.split()]\n",
        "\n",
        "print(\"== Lexical coverage (per dataset) ==\")\n",
        "for f in files:\n",
        "    toks = read_tokens(f)\n",
        "    cov_w2v = lexical_coverage(w2v, toks)\n",
        "    cov_ftv = lexical_coverage(ft, toks)\n",
        "    print(f\"{f}: W2V={cov_w2v:.3f}, FT(vocab)={cov_ftv:.3f}\")\n",
        "\n",
        "def cos(a,b): return float(dot(a,b)/(norm(a)*norm(b)))\n",
        "\n",
        "def pair_sim(model, pairs):\n",
        "    vals = []\n",
        "    for a,b in pairs:\n",
        "        try: vals.append(model.wv.similarity(a,b))\n",
        "        except KeyError: pass\n",
        "    return sum(vals)/len(vals) if vals else float('nan')\n",
        "\n",
        "syn_w2v = pair_sim(w2v, syn_pairs)\n",
        "syn_ft  = pair_sim(ft,  syn_pairs)\n",
        "ant_w2v = pair_sim(w2v, ant_pairs)\n",
        "ant_ft  = pair_sim(ft,  ant_pairs)\n",
        "\n",
        "print(\"\\n== Similarity (higher better for synonyms; lower better for antonyms) ==\")\n",
        "print(f\"Synonyms: W2V={syn_w2v:.3f}, FT={syn_ft:.3f}\")\n",
        "print(f\"Antonyms: W2V={ant_w2v:.3f}, FT={ant_ft:.3f}\")\n",
        "print(f\"Separation (Syn - Ant): W2V={(syn_w2v - ant_w2v):.3f}, FT={(syn_ft - ant_ft):.3f}\")\n",
        "\n",
        "def neighbors(model, word, k=5):\n",
        "    try: return [w for w,_ in model.wv.most_similar(word, topn=k)]\n",
        "    except KeyError: return []\n",
        "\n",
        "print(\"\\n== Nearest neighbors (qualitative) ==\")\n",
        "for w in seed_words:\n",
        "    print(f\"  W2V NN for '{w}':\", neighbors(w2v, w))\n",
        "    print(f\"  FT  NN for '{w}':\", neighbors(ft,  w))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}