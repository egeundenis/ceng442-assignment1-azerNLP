{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R8m7bzAJCmYd"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CENG442 Assignment 1 - Azerbaijani Text Preprocessing + Word Embeddings\n",
        "\"\"\"\n",
        "\n",
        "# --- Core imports ---\n",
        "import re, html, unicodedata\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Data handling ---\n",
        "import pandas as pd\n",
        "\n",
        "# --- Text cleaning ---\n",
        "try:\n",
        "    from ftfy import fix_text\n",
        "except Exception:\n",
        "    def fix_text(s): return s\n",
        "\n",
        "# --- Embeddings ---\n",
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "# --- Utilities ---\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CENG442 Assignment 1 - Azerbaijani Text Preprocessing + Word Embeddings\n",
        "Combined pipeline script (from PDF content)\n",
        "Author(s): <your names>\n",
        "\"\"\"\n",
        "\n",
        "# --- Core imports ---\n",
        "import re\n",
        "import html\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Data handling ---\n",
        "import pandas as pd\n",
        "\n",
        "# --- Text cleaning and normalization ---\n",
        "try:\n",
        "    from ftfy import fix_text\n",
        "except Exception:\n",
        "    def fix_text(s): return s\n",
        "\n",
        "# --- Word embeddings ---\n",
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "# --- Utilities ---\n",
        "from sklearn.model_selection import train_test_split  # optional\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# Azerbaijani-aware lowercase\n",
        "# ---------------------------\n",
        "def lower_az(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    s = s.replace(\"I\", \"ƒ±\").replace(\"ƒ∞\", \"i\")\n",
        "    # normalize dotted i artifacts\n",
        "    s = s.lower().replace(\"i Ãá\",\"i\")\n",
        "    return s\n",
        "\n",
        "# ---------------------------\n",
        "# Regular expressions & maps\n",
        "# ---------------------------\n",
        "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "URL_RE      = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
        "EMAIL_RE    = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", re.IGNORECASE)\n",
        "PHONE_RE    = re.compile(r\"\\+?\\d[\\d\\-\\s\\(\\)]{6,}\\d\")\n",
        "USER_RE     = re.compile(r\"@\\w+\")\n",
        "MULTI_PUNCT = re.compile(r\"([!?.,;:])\\1{1,}\")\n",
        "MULTI_SPACE = re.compile(r\"\\s+\")\n",
        "REPEAT_CHARS= re.compile(r\"(.)\\1{2,}\", flags=re.UNICODE)\n",
        "\n",
        "TOKEN_RE = re.compile(\n",
        "    r\"[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+(?:'[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+)?\"\n",
        "    r\"|<NUM>|URL|EMAIL|PHONE|USER|EMO_(?:POS|NEG)\"\n",
        ")\n",
        "\n",
        "EMO_MAP = {\n",
        "    \"üôÇ\":\"EMO_POS\",\"üòÄ\":\"EMO_POS\",\"üòç\":\"EMO_POS\",\"üòä\":\"EMO_POS\",\"üëç\":\"EMO_POS\",\n",
        "    \"‚òπ\":\"EMO_NEG\",\"üôÅ\":\"EMO_NEG\",\"üò†\":\"EMO_NEG\",\"üò°\":\"EMO_NEG\",\"üëé\":\"EMO_NEG\"\n",
        "}\n",
        "\n",
        "SLANG_MAP = {\"slm\":\"salam\",\"tmm\":\"tamam\",\"sagol\":\"saƒüol\",\"cox\":\"√ßox\",\"yaxsi\":\"yax≈üƒ±\"}\n",
        "NEGATORS  = {\"yox\",\"deyil\",\"he√ß\",\"q…ôtiyy…ôn\",\"yoxdur\"}\n",
        "\n",
        "# ---------------------------\n",
        "# Domain detection helpers\n",
        "# ---------------------------\n",
        "NEWS_HINTS   = re.compile(r\"\\b(apa|trend|azertac|reuters|bloomberg|dha|aa)\\b\", re.I)\n",
        "SOCIAL_HINTS = re.compile(r\"\\b(rt)\\b|@|#|(?:üòÇ|üòç|üòä|üëç|üëé|üò°|üôÇ)\")\n",
        "REV_HINTS    = re.compile(r\"\\b(azn|manat|qiym…ôt|aldƒ±m|ulduz|√ßox yax≈üƒ±|√ßox pis)\\b\", re.I)\n",
        "\n",
        "PRICE_RE     = re.compile(r\"\\b\\d+\\s*(azn|manat)\\b\", re.I)\n",
        "STARS_RE     = re.compile(r\"\\b([1-5])\\s*ulduz\\b\", re.I)\n",
        "POS_RATE     = re.compile(r\"\\b√ßox yax≈üƒ±\\b\")\n",
        "NEG_RATE     = re.compile(r\"\\b√ßox pis\\b\")\n",
        "\n",
        "def detect_domain(text: str) -> str:\n",
        "    s = str(text).lower()\n",
        "    if NEWS_HINTS.search(s): return \"news\"\n",
        "    if SOCIAL_HINTS.search(s): return \"social\"\n",
        "    if REV_HINTS.search(s):   return \"reviews\"\n",
        "    return \"general\"\n",
        "\n",
        "def domain_specific_normalize(cleaned: str, domain: str) -> str:\n",
        "    if domain == \"reviews\":\n",
        "        s = PRICE_RE.sub(\" <PRICE> \", cleaned)\n",
        "        # STARS_RE replacement uses the match number\n",
        "        s = STARS_RE.sub(lambda m: f\" <STARS_{m.group(1)}> \", s)\n",
        "        s = POS_RATE.sub(\" <RATING_POS> \", s)\n",
        "        s = NEG_RATE.sub(\" <RATING_NEG> \", s)\n",
        "        return \" \".join(s.split())\n",
        "    return cleaned\n",
        "\n",
        "def add_domain_tag(line: str, domain: str) -> str:\n",
        "    return f\"dom{domain} \" + line  # e.g., 'domnews ...'\n",
        "\n",
        "# ---------------------------\n",
        "# Normalization function\n",
        "# ---------------------------\n",
        "def normalize_text_az(s: str, numbers_to_token=True, keep_sentence_punct=False) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    # emoji map first\n",
        "    for emo, tag in EMO_MAP.items():\n",
        "        s = s.replace(emo, f\" {tag} \")\n",
        "    s = fix_text(s)\n",
        "    s = html.unescape(s)\n",
        "    s = HTML_TAG_RE.sub(\" \", s)\n",
        "    s = URL_RE.sub(\" URL \", s)\n",
        "    s = EMAIL_RE.sub(\" EMAIL \", s)\n",
        "    s = PHONE_RE.sub(\" PHONE \", s)\n",
        "    # Hashtag: keep text, split camelCase\n",
        "    s = re.sub(r\"#([A-Za-z0-9_]+)\", lambda m: \" \" + re.sub('([a-z])([A-Z])', r'\\1 \\2', m.group(1)) + \" \", s)\n",
        "    s = USER_RE.sub(\" USER \", s)\n",
        "    s = lower_az(s)\n",
        "    s = MULTI_PUNCT.sub(r\"\\1\", s)\n",
        "    if numbers_to_token:\n",
        "        s = re.sub(r\"\\d+\", \" <NUM> \", s)\n",
        "    if keep_sentence_punct:\n",
        "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ.!?]\", \" \", s)\n",
        "    else:\n",
        "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", s)\n",
        "    s = MULTI_SPACE.sub(\" \", s).strip()\n",
        "    toks = TOKEN_RE.findall(s)\n",
        "    norm = []\n",
        "    mark_neg = 0\n",
        "    for t in toks:\n",
        "        t = REPEAT_CHARS.sub(r\"\\1\\1\", t)\n",
        "        t = SLANG_MAP.get(t, t)\n",
        "        if t in NEGATORS:\n",
        "            norm.append(t); mark_neg = 3; continue\n",
        "        if mark_neg > 0 and t not in {\"URL\",\"EMAIL\",\"PHONE\",\"USER\"}:\n",
        "            norm.append(t + \"_NEG\"); mark_neg -= 1\n",
        "        else:\n",
        "            norm.append(t)\n",
        "    norm = [t for t in norm if not (len(t) == 1 and t not in {\"o\",\"e\"})]\n",
        "    return \" \".join(norm).strip()\n",
        "\n",
        "# ---------------------------\n",
        "# Sentiment mapping\n",
        "# ---------------------------\n",
        "def map_sentiment_value(v, scheme: str):\n",
        "    if scheme == \"binary\":\n",
        "        try:\n",
        "            return 1.0 if int(v) == 1 else 0.0\n",
        "        except Exception:\n",
        "            return None\n",
        "    s = str(v).strip().lower()\n",
        "    if s in {\"pos\",\"positive\",\"1\",\"m√ºsb…ôt\",\"good\",\"pozitiv\",\"m√ºsb…ôt\"}: return 1.0\n",
        "    if s in {\"neu\",\"neutral\",\"2\",\"neytral\",\"neutral\"}: return 0.5\n",
        "    if s in {\"neg\",\"negative\",\"0\",\"m…ônfi\",\"bad\",\"neqativ\"}: return 0.0\n",
        "    return None\n",
        "\n",
        "# ---------------------------\n",
        "# File processing pipeline\n",
        "# ---------------------------\n",
        "def process_file(in_path, text_col, label_col, scheme, out_two_col_path, remove_stopwords=False):\n",
        "    df = pd.read_excel(in_path)\n",
        "    for c in [\"Unnamed: 0\",\"index\"]:\n",
        "        if c in df.columns:\n",
        "            df = df.drop(columns=[c])\n",
        "    assert text_col in df.columns and label_col in df.columns, f\"Missing columns in {in_path}\"\n",
        "    # original text kept for domain detection\n",
        "    df = df.dropna(subset=[text_col])\n",
        "    df = df[df[text_col].astype(str).str.strip().str.len() > 0]\n",
        "    df = df.drop_duplicates(subset=[text_col])\n",
        "\n",
        "    # base clean\n",
        "    df[\"cleaned_text\"] = df[text_col].astype(str).apply(lambda s: normalize_text_az(s))\n",
        "    # domain-aware tweak\n",
        "    df[\"__domain__\"] = df[text_col].astype(str).apply(detect_domain)\n",
        "    df[\"cleaned_text\"] = df.apply(lambda r: domain_specific_normalize(r[\"cleaned_text\"], r[\"__domain__\"]), axis=1)\n",
        "\n",
        "    # optional stopwords (kept minimal, sentiment words preserved)\n",
        "    if remove_stopwords:\n",
        "        sw = set([\"v…ô\",\"il…ô\",\"amma\",\"ancaq\",\"lakin\",\"ya\",\"h…ôm\",\"ki\",\"bu\",\"bir\",\"o\",\"biz\",\"siz\",\"m…ôn\",\"s…ôn\",\"orada\",\"burada\",\"b√ºt√ºn\",\"h…ôr\",\"artƒ±q\",\"√ßox\",\"az\",\"…ôn\",\"d…ô\",\"da\",\"√º√ß√ºn\"])\n",
        "        for keep in [\"deyil\",\"yox\",\"he√ß\",\"q…ôtiyy…ôn\",\"yoxdur\"]:\n",
        "            sw.discard(keep)\n",
        "        df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda s: \" \".join([t for t in s.split() if t not in sw]))\n",
        "\n",
        "    # sentiment mapping (0.0 / 0.5 / 1.0)\n",
        "    df[\"sentiment_value\"] = df[label_col].apply(lambda v: map_sentiment_value(v, scheme))\n",
        "    df = df.dropna(subset=[\"sentiment_value\"])\n",
        "    df[\"sentiment_value\"] = df[\"sentiment_value\"].astype(float)\n",
        "\n",
        "    # final two-column output\n",
        "    out_df = df[[\"cleaned_text\",\"sentiment_value\"]].reset_index(drop=True)\n",
        "    Path(out_two_col_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_df.to_excel(out_two_col_path, index=False)\n",
        "    print(f\"Saved: {out_two_col_path} (rows={len(out_df)})\")\n",
        "\n",
        "# ---------------------------\n",
        "# Build combined corpus (domain-tagged)\n",
        "# ---------------------------\n",
        "def build_corpus_txt(input_files, text_cols, out_txt=\"corpus_all.txt\"):\n",
        "    \"\"\"Create domain-tagged, lowercase, punctuation-free corpus (one sentence per line).\"\"\"\n",
        "    lines = []\n",
        "    for (f, text_col) in zip(input_files, text_cols):\n",
        "        df = pd.read_excel(f)\n",
        "        for raw in df[text_col].dropna().astype(str):\n",
        "            dom = detect_domain(raw)\n",
        "            s = normalize_text_az(raw, keep_sentence_punct=True)\n",
        "            parts = re.split(r\"[.!?]+\", s)\n",
        "            for p in parts:\n",
        "                p = p.strip()\n",
        "                if not p: continue\n",
        "                p = re.sub(r\"[^\\w\\s…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", p)  # remove punctuation\n",
        "                p = \" \".join(p.split()).lower()\n",
        "                if p:\n",
        "                    lines.append(f\"dom{dom} \" + p)\n",
        "    Path(out_txt).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(out_txt, \"w\", encoding=\"utf-8\") as w:\n",
        "        for ln in lines:\n",
        "            w.write(ln + \"\\n\")\n",
        "    print(f\"Wrote {out_txt} with {len(lines)} lines\")\n",
        "\n",
        "# ---------------------------\n",
        "# Train Word2Vec and FastText\n",
        "# ---------------------------\n",
        "def train_embeddings(two_col_files, emb_dir=\"embeddings\", vector_size=300, window=5, min_count=3, epochs=10):\n",
        "    Path(emb_dir).mkdir(parents=True, exist_ok=True)\n",
        "    sentences = []\n",
        "    for f in two_col_files:\n",
        "        df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
        "        sentences.extend(df[\"cleaned_text\"].astype(str).str.split().tolist())\n",
        "\n",
        "    print(f\"Total sentences for training: {len(sentences)}\")\n",
        "\n",
        "    # --- Word2Vec ---\n",
        "    w2v = Word2Vec(\n",
        "        sentences=sentences,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        sg=1,\n",
        "        negative=10,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    w2v.save(str(Path(emb_dir) / \"word2vec.model\"))\n",
        "\n",
        "    # --- FastText ---\n",
        "    ft = FastText(\n",
        "        sentences=sentences,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        sg=1,\n",
        "        min_n=3,\n",
        "        max_n=6,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    ft.save(str(Path(emb_dir) / \"fasttext.model\"))\n",
        "\n",
        "    print(\"Saved embeddings.\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Simple comparison utilities\n",
        "# ---------------------------\n",
        "def lexical_coverage(model, tokens):\n",
        "    vocab = model.wv.key_to_index\n",
        "    return sum(1 for t in tokens if t in vocab) / max(1,len(tokens))\n",
        "\n",
        "def pair_sim(model, pairs):\n",
        "    vals = []\n",
        "    for a,b in pairs:\n",
        "        try: vals.append(model.wv.similarity(a,b))\n",
        "        except KeyError: pass\n",
        "    return sum(vals)/len(vals) if vals else float('nan')\n",
        "\n",
        "def neighbors(model, word, k=5):\n",
        "    try: return [w for w,_ in model.wv.most_similar(word, topn=k)]\n",
        "    except KeyError: return []\n",
        "\n",
        "# ---------------------------\n",
        "# Example main (CFG from PDF)\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    CFG = [\n",
        "        (\"sample_data/labeled-sentiment.xlsx\",        \"text\", \"sentiment\", \"tri\"),\n",
        "        (\"sample_data/test__1_.xlsx\",                 \"text\", \"label\",     \"binary\"),\n",
        "        (\"sample_data/train__3_.xlsx\",                \"text\", \"label\",     \"binary\"),\n",
        "        (\"sample_data/train-00000-of-00001.xlsx\",     \"text\", \"labels\",    \"tri\"),\n",
        "        (\"sample_data/merged_dataset_CSV__1_.xlsx\",   \"text\", \"labels\",    \"binary\"),\n",
        "    ]\n",
        "\n",
        "    # If your files are in sample_data/ change the names e.g. \"sample_data/labeled-sentiment.xlsx\"\n",
        "    # Create two-column outputs\n",
        "    out_files = []\n",
        "    for fname, tcol, lcol, scheme in CFG:\n",
        "        out = f\"{Path(fname).stem}_2col.xlsx\"\n",
        "        print(f\"Processing {fname} -> {out}\")\n",
        "        try:\n",
        "            process_file(fname, tcol, lcol, scheme, out, remove_stopwords=False)\n",
        "            out_files.append(out)\n",
        "        except AssertionError as e:\n",
        "            print(f\"Skipping {fname} - {e}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {fname} (skipping). Put your dataset in working dir or update CFG.\")\n",
        "        except Exception as ex:\n",
        "            print(f\"Error processing {fname}: {ex}\")\n",
        "\n",
        "    # combined domain-tagged, punctuation-free corpus\n",
        "    # Use original text columns for corpus creation (CFG entries' text columns)\n",
        "    files_for_corpus = [c[0] for c in CFG]\n",
        "    text_cols_for_corpus = [c[1] for c in CFG]\n",
        "    # Only build corpus if the files exist\n",
        "    existing_pairs = []\n",
        "    for f,tcol in zip(files_for_corpus, text_cols_for_corpus):\n",
        "        if Path(f).exists():\n",
        "            existing_pairs.append((f,tcol))\n",
        "    if existing_pairs:\n",
        "        build_corpus_txt([p[0] for p in existing_pairs], [p[1] for p in existing_pairs], out_txt=\"corpus_all.txt\")\n",
        "    else:\n",
        "        print(\"No input files found for corpus build; place files or adjust CFG.\")\n",
        "\n",
        "    # Train embeddings if two-col outputs exist\n",
        "    two_col_files = [f for f in out_files if Path(f).exists()]\n",
        "    if two_col_files:\n",
        "        train_embeddings(two_col_files, emb_dir=\"embeddings\", vector_size=300, window=5, min_count=3, epochs=10)\n",
        "\n",
        "        # Small comparison example\n",
        "        try:\n",
        "            w2v = Word2Vec.load(\"embeddings/word2vec.model\")\n",
        "            ft  = FastText.load(\"embeddings/fasttext.model\")\n",
        "            seed_words = [\"yax≈üƒ±\",\"pis\",\"√ßox\",\"bahalƒ±\",\"ucuz\",\"m√ºk…ômm…ôl\",\"d…ôh≈ü…ôt\",\"<PRICE>\",\"<RATING_POS>\"]\n",
        "            syn_pairs  = [(\"yax≈üƒ±\",\"…ôla\"), (\"bahalƒ±\",\"qiym…ôtli\"), (\"ucuz\",\"s…ôrf…ôli\")]\n",
        "            ant_pairs  = [(\"yax≈üƒ±\",\"pis\"), (\"bahalƒ±\",\"ucuz\")]\n",
        "            print(\"== Lexical coverage & sample neighbors ==\")\n",
        "            # read tokens from first two-col file as example\n",
        "            sample_tokens = []\n",
        "            if two_col_files:\n",
        "                df = pd.read_excel(two_col_files[0], usecols=[\"cleaned_text\"])\n",
        "                sample_tokens = [t for row in df[\"cleaned_text\"].astype(str) for t in row.split()]\n",
        "            print(f\"Coverage W2V (sample): {lexical_coverage(w2v, sample_tokens):.3f}\")\n",
        "            print(f\"Coverage FT (sample): {lexical_coverage(ft, sample_tokens):.3f}\")\n",
        "            syn_w2v = pair_sim(w2v, syn_pairs)\n",
        "            syn_ft  = pair_sim(ft,  syn_pairs)\n",
        "            ant_w2v = pair_sim(w2v, ant_pairs)\n",
        "            ant_ft  = pair_sim(ft,  ant_pairs)\n",
        "            print(f\"Synonyms: W2V={syn_w2v:.3f}, FT={syn_ft:.3f}\")\n",
        "            print(f\"Antonyms: W2V={ant_w2v:.3f}, FT={ant_ft:.3f}\")\n",
        "            for w in seed_words:\n",
        "                print(f\"  W2V NN for '{w}':\", neighbors(w2v, w))\n",
        "                print(f\"  FT  NN for '{w}':\", neighbors(ft,  w))\n",
        "        except Exception as e:\n",
        "            print(\"Skipping embedding comparison due to error:\", e)\n",
        "    else:\n",
        "        print(\"No two-column outputs produced; skip embedding training.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ5guH7pLg0e",
        "outputId": "9c8a4a3c-d113-4485-a2a9-db3f03e0a27c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing sample_data/labeled-sentiment.xlsx -> labeled-sentiment_2col.xlsx\n",
            "Saved: labeled-sentiment_2col.xlsx (rows=2955)\n",
            "Processing sample_data/test__1_.xlsx -> test__1__2col.xlsx\n",
            "Saved: test__1__2col.xlsx (rows=4198)\n",
            "Processing sample_data/train__3_.xlsx -> train__3__2col.xlsx\n",
            "Saved: train__3__2col.xlsx (rows=19557)\n",
            "Processing sample_data/train-00000-of-00001.xlsx -> train-00000-of-00001_2col.xlsx\n",
            "Saved: train-00000-of-00001_2col.xlsx (rows=41756)\n",
            "Processing sample_data/merged_dataset_CSV__1_.xlsx -> merged_dataset_CSV__1__2col.xlsx\n",
            "Saved: merged_dataset_CSV__1__2col.xlsx (rows=55662)\n",
            "Wrote corpus_all.txt with 124353 lines\n",
            "Total sentences for training: 124128\n",
            "Saved embeddings.\n",
            "== Lexical coverage & sample neighbors ==\n",
            "Coverage W2V (sample): 0.932\n",
            "Coverage FT (sample): 0.932\n",
            "Synonyms: W2V=0.356, FT=0.435\n",
            "Antonyms: W2V=0.335, FT=0.421\n",
            "  W2V NN for 'yax≈üƒ±': ['<RATING_POS>', 'iyi', 'yaxshi', 'yaxsƒ±', 'awsome']\n",
            "  FT  NN for 'yax≈üƒ±': ['yax≈üƒ±ƒ±', 'yax≈üƒ±kƒ±', 'yax≈üƒ±ca', 'yax≈ü', 'yax≈üƒ±ya']\n",
            "  W2V NN for 'pis': ['<RATING_NEG>', 'v…ôrdi≈ül…ôr…ô', 'g√ºnd', '√∂rn…ôk', 'lotulardi']\n",
            "  FT  NN for 'pis': ['piis', 'pi', 'pisdii', 'pisi', 'pis…ô']\n",
            "  W2V NN for '√ßox': ['√ßoox', 'b…ôy…ônilsin', '√ß√∂x', '√ßoxx', '…ôladir']\n",
            "  FT  NN for '√ßox': ['√ßox√ßox', '√ßoxx', '√ßoxh', '√ßo', '√ßoh']\n",
            "  W2V NN for 'bahalƒ±': ['qabardƒ±lƒ±r', 'yaxtalarƒ±', 'portretlerin…ô', 'metallarla', '≈üanslƒ±san']\n",
            "  FT  NN for 'bahalƒ±': ['bahalƒ±ƒ±', 'bahalƒ±sƒ±', 'bahalƒ±q', 'baharlƒ±', 'pahalƒ±']\n",
            "  W2V NN for 'ucuz': ['d√ºz…ôltdirilib', '≈üeytanbazardan', 'qiymete', 'sorbasi', 'alinmis']\n",
            "  FT  NN for 'ucuz': ['ucuza', 'ucuzu', 'ucuzdu', 'ucuzluƒüa', 'ucuzdur']\n",
            "  W2V NN for 'm√ºk…ômm…ôl': ['muk…ômm…ôl', 'm√∂ht…ô≈ü…ômm', 'm√ºk…ômm…ôll', 'k…ôlim…ôyl…ô', 'm√∂hd…ô≈ü…ôm']\n",
            "  FT  NN for 'm√ºk…ômm…ôl': ['m√ºk…ômm…ôll', 'm√ºk…ôm…ôl', 'm√ºk…ômm…ôldi', 'muk…ômm…ôl', 'm√ºk…ômm…ôlsiz']\n",
            "  W2V NN for 'd…ôh≈ü…ôt': ['xal√ßalardan', 'ayranlarƒ±', 't…ôsirlidi', 'birda', 'onag√∂r…ô']\n",
            "  FT  NN for 'd…ôh≈ü…ôt': ['d…ôh≈ü…ôtd√º', 'd…ôh≈ü…ôt…ô', 'd…ôh≈ü…ôti', 'd…ôh≈ü…ôtizm', 'd…ôh≈ü…ôtdi']\n",
            "  W2V NN for '<PRICE>': []\n",
            "  FT  NN for '<PRICE>': ['cruise', 'ahiretteki', 'cokubse', 'dovrune', 'fatihe']\n",
            "  W2V NN for '<RATING_POS>': ['deneyin', 'uygulama', 'oynuyorum', 'indirin', 'internetli']\n",
            "  FT  NN for '<RATING_POS>': ['<RATING_NEG>', 's√ºperr', 'c√∂x', '√ßookk', 's√ºper']\n"
          ]
        }
      ]
    }
  ]
}